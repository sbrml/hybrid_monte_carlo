{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "from sonnet import Linear, AbstractModule, BatchFlatten, reuse_variables\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HmcNet(AbstractModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_units=200,\n",
    "                 prior_sigma=1.,\n",
    "                 mass=1.,\n",
    "                 name=\"hmc_net\"):\n",
    "        \n",
    "        super(HmcNet, self).__init__(name=name)\n",
    "        \n",
    "        self._hidden_units = hidden_units\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.mass = mass\n",
    "        \n",
    "    @reuse_variables\n",
    "    def log_prob(self, labels):\n",
    "        self._ensure_is_connected()\n",
    "        \n",
    "        return tf.reduce_sum(self._likelihood.log_prob(labels))\n",
    "    \n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        \n",
    "        flatten = BatchFlatten()\n",
    "        flattened = flatten(inputs)\n",
    "        \n",
    "        linear1 = Linear(output_size=self._hidden_units)\n",
    "        activations = tf.nn.relu(linear1(flattened))\n",
    "        \n",
    "        linear2 = Linear(output_size=self._hidden_units)\n",
    "        activations = tf.nn.relu(linear2(activations))\n",
    "        \n",
    "        linear_out = Linear(output_size=10) \n",
    "        logits = linear_out(activations)\n",
    "\n",
    "        # Regularising term\n",
    "        self.prior_term = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            self.prior_term += tf.reduce_sum(variable**2)\n",
    "        \n",
    "        self._likelihood = tfd.Categorical(logits=logits)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def potential_energy(self, labels):\n",
    "\n",
    "        return self.log_prob(labels) + (0.5 / self.prior_sigma**2) * self.prior_term\n",
    "\n",
    "\n",
    "    def hamiltonian(self, potential, momenta):\n",
    "\n",
    "        s = 0\n",
    "        \n",
    "        for k, v in momenta.items():\n",
    "            s = s + tf.reduce_sum(v**2)\n",
    "\n",
    "        return potential + 0.5 * s / self.mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(data, labels, batch_size=128, shuffle_buffer=2000):\n",
    "    \n",
    "    mnist_ds = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    mnist_ds = mnist_ds.map(mnist_process)\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=shuffle_buffer)\n",
    "    mnist_ds = mnist_ds.batch(batch_size)\n",
    "    \n",
    "    return mnist_ds\n",
    "\n",
    "def mnist_process(data, label):\n",
    "    \n",
    "    return tf.cast(data, tf.float32) / 255., tf.cast(label, tf.int64)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_data, train_labels),\n",
    "(test_data, test_labels)) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "WARNING:tensorflow:From /Users/gergelyflamich/Documents/sbrml/hybrid_monte_carlo/hmc_venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loss: 241.21, Accuracy: 79.92 \n",
      "Epoch 2: \n",
      "Loss: 169.32, Accuracy: 84.81 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "hmc_net = HmcNet(hidden_units=200)\n",
    "\n",
    "train_dataset = mnist_dataset(train_data, train_labels)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(3e-4)\n",
    "\n",
    "accuracy_metric = tfe.metrics.Accuracy()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    print(\"Epoch {}: \".format(epoch + 1))\n",
    "    for data_batch, label_batch in train_dataset:\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            logits = hmc_net(data_batch)\n",
    "        \n",
    "            loss = -hmc_net.log_prob(label_batch) + hmc_net.prior_term\n",
    "        \n",
    "        grads = tape.gradient(loss, hmc_net.get_all_variables())\n",
    "        optimizer.apply_gradients(zip(grads, hmc_net.get_all_variables()))\n",
    "        \n",
    "        accuracy_metric(labels=label_batch,\n",
    "                        predictions=tf.argmax(logits, axis=1))\n",
    "        \n",
    "        print(\"Loss: {:.2f}, Accuracy: {:.2f} \\r\".format(loss, accuracy_metric.result() * 100), end=\"\")\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9223372036538493538\n",
      "316282260\n",
      "-9223372036538365934\n",
      "-9223372036538365948\n",
      "316409941\n",
      "316409913\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    hmc_net(data_batch)\n",
    "    pe = hmc_net.potential_energy(label_batch)\n",
    "    \n",
    "    \n",
    "for var in hmc_net.get_all_variables():\n",
    "    print(hash(var))\n",
    "    \n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dynamics(hmc_net, momenta, x_train, y_train, epsilon, num_steps):\n",
    "    \"\"\"\n",
    "    Leapfrog integration of the dynamics for\n",
    "\n",
    "    time = num_steps * epsilon\n",
    "    \"\"\"\n",
    "\n",
    "    # Computing dE_dz\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        logits = hmc_net(x_train)\n",
    "        E = hmc_net.potential_energy(y_train)\n",
    "\n",
    "    # Sample integration direction\n",
    "    epsilon = epsilon * (2 * tf.cast(tf.random.categorical(tf.log([[0.5, 0.5]]), 1), tf.float32) - 1)[0, 0]\n",
    "\n",
    "    # First leapfrog update\n",
    "    for var in tf.trainable_variables():\n",
    "        momenta[var] -= (epsilon / 2.) * tape.gradient(E, var)\n",
    "\n",
    "    # delete tape here\n",
    "    del tape\n",
    "\n",
    "    for i in range(num_steps - 1):\n",
    "\n",
    "        # Middle leapfrog steps\n",
    "        # z = z + epsilon * r\n",
    "        for var in tf.trainable_variables():\n",
    "            var.assign_add(epsilon * momenta[var] / hmc_net.mass)\n",
    "\n",
    "        # Computing dE_dz\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            logits = hmc_net(x_train)\n",
    "            E = hmc_net.potential_energy(y_train)\n",
    "\n",
    "        # r = r - epsilon * dE_dz(z)\n",
    "        for var in tf.trainable_variables():\n",
    "            momenta[var] -= epsilon * tape.gradient(E, var)\n",
    "\n",
    "        # delete tape here\n",
    "        del tape\n",
    "\n",
    "    # Final leapfrog steps\n",
    "    # z = z + epsilon * r\n",
    "    for var in tf.trainable_variables():\n",
    "        var.assign_add(epsilon * tf.squeeze(momenta[var]) / hmc_net.mass)\n",
    "\n",
    "    # Computing dE_dz\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        logits = hmc_net(x_train)\n",
    "        E = hmc_net.potential_energy(y_train)\n",
    "\n",
    "    # r = r - epsilon * dE_dz(z)\n",
    "    for var in tf.trainable_variables():\n",
    "        momenta[var] -= (epsilon / 2.) * tape.gradient(E, var)\n",
    "\n",
    "    # delete tape here\n",
    "    del tape\n",
    "\n",
    "    return momenta\n",
    "\n",
    "\n",
    "def hmc_sample(hmc_net, x_train, y_train, mixing_time=50, burn_in_time=10,\n",
    "               num_integral_steps=10, num_samples=100, epsilon=1e-3, log_every=1):\n",
    "\n",
    "    num_iters = num_samples * mixing_time + burn_in_time\n",
    "    num_accepted = 0\n",
    "\n",
    "    accuracy_metric = tfe.metrics.Accuracy()\n",
    "    \n",
    "    for i in tqdm(range(num_iters)):\n",
    "\n",
    "        vars_0 = {v: tf.identity(v) for v in tf.trainable_variables()}\n",
    "        momenta_0 = {v: tf.random.normal(v.shape, stddev=hmc_net.mass**0.5)\n",
    "                     for v in tf.trainable_variables()}\n",
    "        \n",
    "\n",
    "        hmc_net(x_train)\n",
    "        H_0 = hmc_net.hamiltonian(hmc_net.potential_energy(y_train), momenta_0)\n",
    "\n",
    "        momenta = run_dynamics(hmc_net=hmc_net,\n",
    "                               momenta=momenta_0,\n",
    "                               x_train=x_train,\n",
    "                               y_train=y_train,\n",
    "                               epsilon=epsilon,\n",
    "                               num_steps=num_integral_steps)\n",
    "\n",
    "        hmc_net(x_train)\n",
    "        H = hmc_net.hamiltonian(hmc_net.potential_energy(y_train), momenta)\n",
    "\n",
    "        threshold = tf.minimum(1, tf.exp(H_0 - H))\n",
    "\n",
    "        u = tf.random.uniform(shape=(1,), minval=0., maxval=1.)\n",
    "        is_accepted = u <= threshold\n",
    "\n",
    "        if not is_accepted:\n",
    "            for var in tf.trainable_variables():\n",
    "                var.assign(vars_0[var])\n",
    "        else:\n",
    "            num_accepted += 1\n",
    "\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            predictions = tf.argmax(hmc_net(x_train), axis=1)\n",
    "            accuracy_metric(predictions=predictions,\n",
    "                            labels=y_train)\n",
    "\n",
    "            print('Train accuracy {:.3}, accepted {} out of {}'.format(accuracy_metric.result(), num_accepted, i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6887e6a17ee49f484addb89ac765063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=510), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hmc_net = HmcNet(hidden_units=100, mass=1e-2)\n",
    "\n",
    "train_dataset = mnist_dataset(train_data, train_labels, batch_size=128)\n",
    "\n",
    "for data_batch, label_batch in train_dataset.take(1):\n",
    "    hmc_net(data_batch)\n",
    "\n",
    "    hmc_sample(hmc_net=hmc_net,\n",
    "               x_train=data_batch, \n",
    "               y_train=label_batch,\n",
    "               mixing_time=100, \n",
    "               burn_in_time=10,\n",
    "               num_integral_steps=10, \n",
    "               num_samples=5, \n",
    "               epsilon=1e-4, \n",
    "               log_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
